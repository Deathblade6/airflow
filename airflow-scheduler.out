[[34m2021-04-05 05:10:12,914[0m] {[34mscheduler_job.py:[0m1241} INFO[0m - Starting the scheduler[0m
[[34m2021-04-05 05:10:12,918[0m] {[34mscheduler_job.py:[0m1246} INFO[0m - Processing each file at most -1 times[0m
[[34m2021-04-05 05:10:13,474[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 1957[0m
[[34m2021-04-05 05:10:13,489[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:10:13,647[0m] {[34mscheduler_job.py:[0m1764} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2021-04-05 05:10:13,731[0m] {[34msettings.py:[0m52} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-05 05:10:13,849[0m] {[34mscheduler_job.py:[0m1804} INFO[0m - Reset the following 18 orphaned TaskInstances:
	<TaskInstance: dumb_me.main 2021-04-02 04:50:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 05:40:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 07:40:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 07:30:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 06:40:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 06:30:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 06:20:00+00:00 [queued]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 13:10:00+00:00 [queued]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 15:20:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 04:40:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 07:50:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 07:20:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 07:10:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 07:00:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 06:50:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 06:10:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 06:00:00+00:00 [queued]>
	<TaskInstance: dumb_me.main 2021-04-02 05:50:00+00:00 [queued]>[0m
[[34m2021-04-05 05:10:15,519[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 20 tasks up for execution:
	<TaskInstance: dumb_me.main 2021-04-02 04:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 05:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 13:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 15:20:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 05:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:00:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:10:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:20:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:30:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:40:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:00:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:10:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:20:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:30:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 15:30:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.bash4 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.downloading_data 2021-04-02 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 20 task instances ready to be queued[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 2/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 3/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,539[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 4/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 5/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 6/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 7/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 8/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 9/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 10/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 11/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 12/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 13/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 14/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 2/16 running and queued tasks[0m
[[34m2021-04-05 05:10:15,540[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag.downloading_data 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.bash4 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 04:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 05:40:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 05:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:00:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:10:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:20:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:30:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:40:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 06:50:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:00:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:10:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:20:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:30:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:40:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 07:50:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 13:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 15:20:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-02 15:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:15,591[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='downloading_data', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 8 and queue default[0m
[[34m2021-04-05 05:10:15,591[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'downloading_data', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:15,592[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='bash4', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-04-05 05:10:15,592[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'bash4', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:15,592[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 4, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,592[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,592[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,593[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,593[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,593[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,593[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,593[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,593[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,593[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,593[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,594[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,594[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,594[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,594[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,594[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,594[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,594[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,594[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,594[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,594[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,595[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,595[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,595[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,595[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 7, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,595[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,595[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 7, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,595[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,595[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 7, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,595[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,595[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 13, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,595[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-02T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,596[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 15, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,596[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-02T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,596[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 15, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:15,596[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-02T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,645[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'downloading_data', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:15,652[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'bash4', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:15,655[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T04:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,659[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,662[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,664[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,666[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,669[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,680[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,682[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,685[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,712[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,714[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,721[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,724[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,743[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,751[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T07:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,753[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-02T13:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,772[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-02T15:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:15,774[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-02T15:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:17,241[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,306[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,311[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,391[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:17,513[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,523[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,535[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,408[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,541[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,550[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,558[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:17,421[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,446[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,568[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,585[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,485[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,487[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,495[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,730[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:17,761[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: xcom_dag.downloading_data 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: parallel_dag.bash4 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,357[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,381[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,531[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,583[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,588[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,630[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,638[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,646[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,656[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,681[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,759[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,786[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,797[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:19,780[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
[[34m2021-04-05 05:10:19,958[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:20,140[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 4 tasks up for execution:
	<TaskInstance: dumb_me.main 2021-04-02 04:40:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.bash4 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.downloading_data 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:20,179[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 108 open slots and 4 task instances ready to be queued[0m
[[34m2021-04-05 05:10:20,189[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:20,189[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:20,189[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG dumb_me has 15/16 running and queued tasks[0m
[[34m2021-04-05 05:10:20,189[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 3/16 running and queued tasks[0m
[[34m2021-04-05 05:10:20,189[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag.downloading_data 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.bash4 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: dumb_me.main 2021-04-02 04:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:20,221[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='downloading_data', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 8 and queue default[0m
[[34m2021-04-05 05:10:20,228[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'downloading_data', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:20,228[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='bash4', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-04-05 05:10:20,229[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'bash4', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:20,229[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='dumb_me', task_id='main', execution_date=datetime.datetime(2021, 4, 2, 4, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:20,229[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:20,229[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:20,229[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:20,236[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'downloading_data', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:20,238[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'bash4', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:20,241[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'dumb_me', 'main', '2021-04-02T04:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:20,246[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:10:20,302[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 04:50:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 05:50:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 07:30:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 07:40:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 06:20:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 06:40:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 07:10:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 06:30:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 06:00:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 06:10:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 05:40:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 07:50:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,303[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 07:20:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,304[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 07:00:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:20,304[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 06:50:00+00:00 exited with status failed for try_number 1[0m
Running <TaskInstance: fec_lead_importer.main 2021-04-02T15:30:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:20,398[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 04:50:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
Running <TaskInstance: fec_lead_importer.main 2021-04-02T13:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:20,437[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 05:40:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,438[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 06:00:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,439[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 07:40:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,439[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 07:30:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,452[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 06:40:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,454[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 06:30:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,504[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 06:20:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,505[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 07:50:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,506[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 07:20:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,518[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 07:10:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,519[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 07:00:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,519[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 06:50:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,519[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 06:10:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
[[34m2021-04-05 05:10:20,520[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 05:50:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
Running <TaskInstance: fec_lead_importer.main 2021-04-02T15:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:21,317[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:21,345[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:21,352[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:10:21,399[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
Running <TaskInstance: xcom_dag.downloading_data 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: parallel_dag.bash4 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:22,616[0m] {[34mlocal_executor.py:[0m127} ERROR[0m - Failed to execute task dag_id could not be found: dumb_me. Either the dag did not exist or it failed to parse..[0m
[[34m2021-04-05 05:10:23,039[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of dumb_me.main execution_date=2021-04-02 04:40:00+00:00 exited with status failed for try_number 1[0m
[[34m2021-04-05 05:10:23,139[0m] {[34mscheduler_job.py:[0m1229} ERROR[0m - Executor reports task instance <TaskInstance: dumb_me.main 2021-04-02 04:40:00+00:00 [queued]> finished (failed) although the task says its queued. (Info: None) Was the task killed externally?[0m
Running <TaskInstance: fec_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:26,042[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-02 15:30:00+00:00: scheduled__2021-04-02T15:30:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:10:26,089[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-02 15:20:00+00:00: scheduled__2021-04-02T15:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:10:26,138[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-02 13:10:00+00:00: scheduled__2021-04-02T13:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:10:26,414[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 5 tasks up for execution:
	<TaskInstance: parallel_dag.processing_tasks.bash3 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash2 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_c 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_b 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_a 2021-04-02 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:26,436[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 109 open slots and 5 task instances ready to be queued[0m
[[34m2021-04-05 05:10:26,436[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:26,436[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 2/16 running and queued tasks[0m
[[34m2021-04-05 05:10:26,436[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 3/16 running and queued tasks[0m
[[34m2021-04-05 05:10:26,436[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:26,436[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 2/16 running and queued tasks[0m
[[34m2021-04-05 05:10:26,437[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag.processing_tasks.training_model_c 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_b 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_a 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash3 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash2 2021-04-02 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:26,447[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='processing_tasks.training_model_c', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 5 and queue default[0m
[[34m2021-04-05 05:10:26,447[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_c', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:26,448[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='processing_tasks.training_model_b', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 5 and queue default[0m
[[34m2021-04-05 05:10:26,448[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_b', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:26,448[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='processing_tasks.training_model_a', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 5 and queue default[0m
[[34m2021-04-05 05:10:26,448[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_a', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:26,448[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='processing_tasks.bash3', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-05 05:10:26,448[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash3', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:26,448[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='processing_tasks.bash2', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-05 05:10:26,448[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash2', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:26,455[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_c', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:26,458[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_b', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:26,461[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_a', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:26,464[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash3', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:26,466[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash2', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:26,470[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-02 15:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:26,496[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-02 15:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:26,496[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-02 13:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:26,496[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.bash4 execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:26,496[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.downloading_data execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:26,556[0m] {[34mdag_processing.py:[0m396} WARNING[0m - DagFileProcessorManager (PID=1957) exited with exit code 1 - re-launching[0m
[[34m2021-04-05 05:10:26,580[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 2315[0m
[[34m2021-04-05 05:10:26,626[0m] {[34msettings.py:[0m52} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-05 05:10:27,054[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:27,074[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:27,085[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:27,093[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:27,100[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:27,668[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 05:00:00+00:00: scheduled__2021-04-05T05:00:00+00:00, externally triggered: False> failed[0m
Running <TaskInstance: parallel_dag.processing_tasks.bash3 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.processing_tasks.training_model_c 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: parallel_dag.processing_tasks.bash2 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.processing_tasks.training_model_a 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.processing_tasks.training_model_b 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:28,284[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:30,859[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 6 tasks up for execution:
	<TaskInstance: xcom_dag.task_4 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash3 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash2 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_c 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_b 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_a 2021-04-04 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 110 open slots and 6 task instances ready to be queued[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 2/16 running and queued tasks[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 3/16 running and queued tasks[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 2/16 running and queued tasks[0m
[[34m2021-04-05 05:10:30,881[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 3/16 running and queued tasks[0m
[[34m2021-04-05 05:10:30,882[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag.processing_tasks.training_model_c 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_b 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.processing_tasks.training_model_a 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: xcom_dag.task_4 2021-04-02 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash3 2021-04-04 00:00:00+00:00 [scheduled]>
	<TaskInstance: parallel_dag.processing_tasks.bash2 2021-04-04 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:30,900[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='processing_tasks.training_model_c', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 5 and queue default[0m
[[34m2021-04-05 05:10:30,900[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_c', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,900[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='processing_tasks.training_model_b', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 5 and queue default[0m
[[34m2021-04-05 05:10:30,900[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_b', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,900[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='processing_tasks.training_model_a', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 5 and queue default[0m
[[34m2021-04-05 05:10:30,900[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_a', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,900[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='task_4', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-04-05 05:10:30,901[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'task_4', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,901[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='processing_tasks.bash3', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-05 05:10:30,901[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash3', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:30,901[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='processing_tasks.bash2', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 2 and queue default[0m
[[34m2021-04-05 05:10:30,901[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash2', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:30,912[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_c', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,915[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_b', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,917[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'processing_tasks.training_model_a', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,919[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'task_4', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:30,920[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash3', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:30,922[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'processing_tasks.bash2', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:30,961[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.bash4 execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:30,961[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.downloading_data execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:30,961[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.processing_tasks.training_model_c execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:30,961[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.processing_tasks.training_model_a execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:30,962[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.processing_tasks.training_model_b execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:31,726[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:31,737[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:31,751[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:31,764[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:31,770[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:31,797[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
Running <TaskInstance: parallel_dag.processing_tasks.bash2 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: parallel_dag.processing_tasks.bash3 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.task_4 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.processing_tasks.training_model_c 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.processing_tasks.training_model_a 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: xcom_dag.processing_tasks.training_model_b 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:37,188[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: xcom_dag.task_4 2021-04-04 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:37,255[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 109 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:10:37,255[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG xcom_dag has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:10:37,255[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: xcom_dag.task_4 2021-04-04 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:37,287[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='xcom_dag', task_id='task_4', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 4 and queue default[0m
[[34m2021-04-05 05:10:37,288[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'xcom_dag', 'task_4', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:37,291[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'xcom_dag', 'task_4', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/xcom_dag.py'][0m
[[34m2021-04-05 05:10:37,293[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.processing_tasks.bash3 execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:38,021[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/xcom_dag.py[0m
[[34m2021-04-05 05:10:38,841[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.processing_tasks.training_model_a execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:38,841[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.task_4 execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:38,842[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.processing_tasks.training_model_b execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:38,842[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.processing_tasks.training_model_c execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
Running <TaskInstance: xcom_dag.task_4 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:40,973[0m] {[34mdagrun.py:[0m444} INFO[0m - Marking run <DagRun xcom_dag @ 2021-04-02 00:00:00+00:00: scheduled__2021-04-02T00:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-05 05:10:41,228[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.processing_tasks.bash3 execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:43,206[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of xcom_dag.task_4 execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:44,862[0m] {[34mdagrun.py:[0m444} INFO[0m - Marking run <DagRun xcom_dag @ 2021-04-04 00:00:00+00:00: scheduled__2021-04-04T00:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-05 05:10:49,467[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: parallel_dag.bash1 2021-04-02 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:49,479[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 111 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:10:49,479[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:49,479[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: parallel_dag.bash1 2021-04-02 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:49,495[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='bash1', execution_date=datetime.datetime(2021, 4, 2, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:49,495[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'bash1', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:49,510[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'bash1', '2021-04-02T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:49,793[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:50,085[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.processing_tasks.bash2 execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
Running <TaskInstance: parallel_dag.bash1 2021-04-02T00:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:54,022[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: parallel_dag.bash1 2021-04-04 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:54,050[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 111 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:10:54,050[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG parallel_dag has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:10:54,050[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: parallel_dag.bash1 2021-04-04 00:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:10:54,058[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='parallel_dag', task_id='bash1', execution_date=datetime.datetime(2021, 4, 4, 0, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:10:54,058[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'parallel_dag', 'bash1', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:54,061[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'parallel_dag', 'bash1', '2021-04-04T00:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/parallel_dag.py'][0m
[[34m2021-04-05 05:10:54,067[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.processing_tasks.bash2 execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:10:54,907[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/parallel_dag.py[0m
[[34m2021-04-05 05:10:55,016[0m] {[34mdagrun.py:[0m444} INFO[0m - Marking run <DagRun parallel_dag @ 2021-04-02 00:00:00+00:00: scheduled__2021-04-02T00:00:00+00:00, externally triggered: False> successful[0m
Running <TaskInstance: parallel_dag.bash1 2021-04-04T00:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:10:56,208[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.bash1 execution_date=2021-04-02 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:11:00,300[0m] {[34mdagrun.py:[0m444} INFO[0m - Marking run <DagRun parallel_dag @ 2021-04-04 00:00:00+00:00: scheduled__2021-04-04T00:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-05 05:11:00,412[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of parallel_dag.bash1 execution_date=2021-04-04 00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:13:47,175[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:13:47,179[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:13:47,179[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:13:47,180[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:13:47,188[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:13:47,189[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:13:47,190[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:13:47,440[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:13:51,494[0m] {[34mdagrun.py:[0m444} INFO[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:00:00+00:00: scheduled__2021-04-05T05:00:00+00:00, externally triggered: False> successful[0m
[[34m2021-04-05 05:13:51,765[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:14:36,871[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:14:36,886[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:14:36,887[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:14:36,887[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:14:36,907[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:14:36,907[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:14:36,913[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:14:37,488[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:14:40,457[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 05:15:14,154[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:16:48,124[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:16:48,138[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:16:48,138[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:16:48,139[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:16:48,140[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=3) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:16:48,141[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:16:48,143[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:16:48,451[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:16:59,436[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 3[0m
[[34m2021-04-05 05:18:42,699[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:18:42,714[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:18:42,714[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:18:42,714[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:18:42,735[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=4) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:18:42,736[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:18:42,738[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:18:43,366[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:18:54,503[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:00:00+00:00: scheduled__2021-04-05T05:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:18:54,799[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 4[0m
[[34m2021-04-05 05:19:57,465[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:19:57,472[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:19:57,472[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:19:57,472[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:19:57,478[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=5) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:19:57,478[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:19:57,481[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:19:57,733[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:20:01,206[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:20:01,211[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 111 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 05:20:01,211[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:20:01,211[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:20:01,211[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:20:01,216[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:20:01,217[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:20:01,217[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:20:01,217[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:20:01,223[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:20:01,226[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:20:01,603[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:20:01,610[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:10:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T05:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:20:04,392[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 05:10:00+00:00: scheduled__2021-04-05T05:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:20:04,926[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:20:08,882[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 5[0m
[[34m2021-04-05 05:20:13,214[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:20:14,371[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:22:52,074[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:22:52,116[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 05:22:52,117[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:22:52,117[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 1/16 running and queued tasks[0m
[[34m2021-04-05 05:22:52,117[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:22:52,143[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=6) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:22:52,144[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:22:52,144[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 10, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:22:52,144[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:22:52,148[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:22:52,159[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:22:52,854[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 05:22:52,856[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:23:04,294[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:10:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 05:23:04,295[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 6[0m
[[34m2021-04-05 05:23:05,387[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:10:00+00:00: scheduled__2021-04-05T05:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:23:05,411[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:00:00+00:00: scheduled__2021-04-05T05:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:23:16,327[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:23:16,347[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:23:16,347[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:23:16,347[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:23:16,350[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 0, tzinfo=Timezone('UTC')), try_number=7) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:23:16,350[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:23:16,353[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:23:16,723[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:23:25,327[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:23:25,336[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:23:25,336[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:23:25,337[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:23:25,348[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 10, tzinfo=Timezone('UTC')), try_number=3) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:23:25,348[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:23:25,353[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:23:25,660[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:23:28,977[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:00:00+00:00: scheduled__2021-04-05T05:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:23:29,324[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:00:00+00:00 exited with status success for try_number 7[0m
[[34m2021-04-05 05:23:37,241[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:10:00+00:00 exited with status success for try_number 3[0m
[[34m2021-04-05 05:23:42,836[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:23:42,840[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:23:42,840[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:23:42,841[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:23:42,855[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 10, tzinfo=Timezone('UTC')), try_number=4) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:23:42,855[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:23:42,857[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:23:43,159[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:23:54,332[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:10:00+00:00: scheduled__2021-04-05T05:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:23:54,547[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:10:00+00:00 exited with status success for try_number 4[0m
[[34m2021-04-05 05:25:14,721[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:30:01,272[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:20:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:30:01,277[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 05:30:01,278[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:30:01,278[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:30:01,278[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:20:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:30:01,281[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:30:01,281[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:30:01,281[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:30:01,281[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:30:01,283[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:30:01,288[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:30:01,581[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 05:30:01,594[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:20:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T05:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:30:02,909[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 05:20:00+00:00: scheduled__2021-04-05T05:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:30:03,173[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:30:03,749[0m] {[34mdag_processing.py:[0m396} WARNING[0m - DagFileProcessorManager (PID=2315) exited with exit code 1 - re-launching[0m
[[34m2021-04-05 05:30:03,754[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 53811[0m
[[34m2021-04-05 05:30:03,781[0m] {[34msettings.py:[0m52} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-05 05:30:12,892[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:30:15,304[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:30:18,039[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:30:18,048[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:30:18,049[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:30:18,049[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:30:18,051[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 20, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:30:18,051[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:30:18,053[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:30:18,343[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:30:29,595[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:20:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 05:30:30,383[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:20:00+00:00: scheduled__2021-04-05T05:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:35:15,456[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:40:00,924[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:30:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:40:00,935[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 05:40:00,936[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:40:00,936[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:40:00,936[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:30:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:40:00,939[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:40:00,939[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:40:00,939[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:40:00,939[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:40:00,949[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:40:00,951[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:40:01,307[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 05:40:01,318[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:30:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T05:30:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:40:03,092[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:40:04,204[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 05:30:00+00:00: scheduled__2021-04-05T05:30:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:40:13,095[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:40:15,767[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:40:18,313[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:40:18,324[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:40:18,324[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:40:18,324[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:40:18,329[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 30, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:40:18,330[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:40:18,331[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:40:18,569[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:30:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:40:29,751[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:30:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 05:40:29,931[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:30:00+00:00: scheduled__2021-04-05T05:30:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:45:15,952[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:50:00,862[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:50:00,863[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 05:50:00,863[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:50:00,863[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:50:00,863[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:50:00,870[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:50:00,870[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:50:00,870[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:50:00,870[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:50:00,872[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:50:00,874[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 05:50:01,122[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 05:50:01,132[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:40:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T05:40:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:50:03,827[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 05:40:00+00:00: scheduled__2021-04-05T05:40:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:50:04,089[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:50:05,179[0m] {[34mdag_processing.py:[0m396} WARNING[0m - DagFileProcessorManager (PID=53811) exited with exit code 1 - re-launching[0m
[[34m2021-04-05 05:50:05,182[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 109772[0m
[[34m2021-04-05 05:50:05,191[0m] {[34msettings.py:[0m52} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-05 05:50:13,971[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 05:50:16,360[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 05:50:17,909[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:50:17,919[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 05:50:17,920[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 05:50:17,921[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 05:50:17,935[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 40, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 05:50:17,935[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:50:17,940[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 05:50:18,292[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:40:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 05:50:30,065[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:40:00+00:00: scheduled__2021-04-05T05:40:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 05:50:30,316[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:40:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 05:55:16,687[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:00:00,595[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:50:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:00:00,603[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 06:00:00,603[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:00:00,603[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:00:00,604[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:50:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:00:00,614[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:00:00,614[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:00:00,617[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:00:00,617[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:00:00,620[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:00:00,622[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:00:00,964[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 06:00:00,968[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:50:00+00:00 [queued]> on host airflowvm
Running <TaskInstance: fec_lead_importer.main 2021-04-05T05:50:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:00:14,732[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:00:15,867[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 05:50:00+00:00: scheduled__2021-04-05T05:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:00:17,034[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:00:25,729[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:00:30,809[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:00:30,812[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 06:00:30,812[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:00:30,812[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 05:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:00:30,818[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 5, 50, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:00:30,818[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:00:30,820[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T05:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:00:31,101[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T05:50:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:00:42,303[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 05:50:00+00:00: scheduled__2021-04-05T05:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:00:42,543[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 05:50:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 06:05:17,088[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:10:01,660[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:00:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:10:01,669[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 06:10:01,669[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:10:01,669[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:10:01,669[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:00:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:10:01,673[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:10:01,673[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:10:01,673[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:10:01,673[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:10:01,676[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:10:01,681[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:10:01,980[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 06:10:01,991[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:00:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T06:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:10:04,436[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 06:00:00+00:00: scheduled__2021-04-05T06:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:10:04,690[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:10:13,213[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:10:17,198[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:10:19,163[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:10:19,168[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 06:10:19,169[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:10:19,169[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:10:19,181[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 0, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:10:19,182[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:10:19,189[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:10:19,618[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:10:30,700[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:00:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 06:10:31,792[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 06:00:00+00:00: scheduled__2021-04-05T06:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:15:17,549[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:20:00,942[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:20:00,946[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 06:20:00,946[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:20:00,950[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:20:00,950[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:20:00,957[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:20:00,958[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:20:00,958[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:20:00,958[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:20:00,962[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:20:00,965[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:20:01,328[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 06:20:01,346[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:10:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T06:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:20:03,306[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:20:04,394[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 06:10:00+00:00: scheduled__2021-04-05T06:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:20:12,624[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:20:17,868[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:20:17,877[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 06:20:17,878[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:20:17,878[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:20:17,883[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 10, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:20:17,883[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:20:17,886[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:20:17,916[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:20:18,145[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:20:29,768[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 06:10:00+00:00: scheduled__2021-04-05T06:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:20:30,075[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:10:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 06:25:18,078[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:30:00,910[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:20:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:30:00,917[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 06:30:00,917[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:30:00,917[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:30:00,917[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:20:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:30:00,919[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:30:00,919[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:30:00,919[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:30:00,919[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:30:00,921[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:30:00,927[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:30:01,234[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 06:30:01,239[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:20:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T06:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:30:04,162[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 06:20:00+00:00: scheduled__2021-04-05T06:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:30:04,641[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:30:05,840[0m] {[34mdag_processing.py:[0m396} WARNING[0m - DagFileProcessorManager (PID=109772) exited with exit code 1 - re-launching[0m
[[34m2021-04-05 06:30:05,843[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 219544[0m
[[34m2021-04-05 06:30:05,864[0m] {[34msettings.py:[0m52} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-05 06:30:13,930[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:30:18,363[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:30:18,366[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 06:30:18,366[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:30:18,370[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:30:18,375[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 20, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:30:18,375[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:30:18,381[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:30:18,396[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:30:18,682[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:30:30,183[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 06:20:00+00:00: scheduled__2021-04-05T06:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:30:30,439[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:20:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 06:35:18,792[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:40:01,463[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:30:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:40:01,472[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 06:40:01,472[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:40:01,472[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:40:01,472[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:30:00+00:00 [scheduled]>
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:40:01,476[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:40:01,478[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:40:01,478[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 30, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:40:01,478[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:40:01,479[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:40:01,482[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:40:01,856[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 06:40:01,874[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:30:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T06:30:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:40:04,339[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 06:30:00+00:00: scheduled__2021-04-05T06:30:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:40:04,546[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:40:13,767[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:30:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:40:19,127[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:40:19,131[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 06:40:19,132[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:40:19,132[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:30:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:40:19,144[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 30, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:40:19,144[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:40:19,146[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:30:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:40:19,198[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:40:19,496[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:30:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:40:31,009[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 06:30:00+00:00: scheduled__2021-04-05T06:30:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:40:31,297[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:30:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 06:45:19,469[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:50:00,559[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:50:00,563[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 06:50:00,563[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:50:00,563[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:50:00,563[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:40:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:50:00,571[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:50:00,571[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:50:00,571[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 40, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:50:00,571[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:50:00,579[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:50:00,581[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 06:50:00,940[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 06:50:00,950[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:40:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T06:40:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:50:02,910[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:50:04,233[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 06:40:00+00:00: scheduled__2021-04-05T06:40:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:50:13,415[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:40:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 06:50:17,384[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:50:17,396[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 06:50:17,397[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 06:50:17,397[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:40:00+00:00 [scheduled]>[0m
[[34m2021-04-05 06:50:17,398[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 40, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 06:50:17,399[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:50:17,400[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:40:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 06:50:17,657[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:40:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 06:50:19,786[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 06:50:28,577[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:40:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 06:50:29,148[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 06:40:00+00:00: scheduled__2021-04-05T06:40:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 06:55:19,966[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:00:01,182[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:50:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:00:01,191[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 07:00:01,191[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:00:01,191[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:00:01,191[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:50:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:00:01,200[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:00:01,201[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:00:01,201[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 50, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:00:01,201[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:00:01,203[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:00:01,206[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:00:01,553[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 07:00:01,572[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:50:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T06:50:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:00:02,965[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 06:50:00+00:00: scheduled__2021-04-05T06:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:00:03,231[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:00:13,088[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:50:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:00:18,571[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:00:18,583[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 07:00:18,583[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:00:18,583[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 06:50:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:00:18,588[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 6, 50, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:00:18,588[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:00:18,591[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T06:50:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:00:18,927[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T06:50:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:00:20,380[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:00:30,412[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 06:50:00+00:00: scheduled__2021-04-05T06:50:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:00:30,875[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 06:50:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 07:05:20,834[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:10:00,772[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:00:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:10:00,776[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 07:10:00,776[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:10:00,776[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:10:00,776[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:00:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:10:00,792[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:10:00,792[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:10:00,792[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 0, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:10:00,792[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:10:00,795[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:10:00,797[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:10:01,177[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 07:10:01,164[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T07:00:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T07:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:10:04,101[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 07:00:00+00:00: scheduled__2021-04-05T07:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:10:04,539[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:10:12,825[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 07:00:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:10:17,818[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:10:17,824[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 07:10:17,825[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:10:17,825[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:00:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:10:17,831[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 0, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:10:17,831[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:10:17,833[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:00:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:10:18,119[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T07:00:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:10:21,124[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:10:29,520[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 07:00:00+00:00: scheduled__2021-04-05T07:00:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:10:29,804[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 07:00:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 07:15:21,411[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:20:01,293[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:20:01,301[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 07:20:01,301[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:20:01,301[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:20:01,301[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:10:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:20:01,307[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:20:01,308[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:20:01,308[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 10, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:20:01,308[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:20:01,311[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:20:01,313[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:20:01,649[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
[[34m2021-04-05 07:20:01,666[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T07:10:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T07:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:20:03,499[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:20:04,452[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 07:10:00+00:00: scheduled__2021-04-05T07:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:20:06,335[0m] {[34mdag_processing.py:[0m396} WARNING[0m - DagFileProcessorManager (PID=219544) exited with exit code 1 - re-launching[0m
[[34m2021-04-05 07:20:06,338[0m] {[34mdag_processing.py:[0m250} INFO[0m - Launched DagFileProcessorManager with pid: 353813[0m
[[34m2021-04-05 07:20:06,348[0m] {[34msettings.py:[0m52} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2021-04-05 07:20:14,812[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 07:10:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:20:18,605[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:20:18,607[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 07:20:18,607[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:20:18,607[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:10:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:20:18,615[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 10, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:20:18,615[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:20:18,617[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:10:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:20:18,944[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T07:10:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:20:21,826[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:20:30,293[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 07:10:00+00:00: scheduled__2021-04-05T07:10:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:20:30,539[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 07:10:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 07:25:22,179[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:30:01,299[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 2 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:20:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:30:01,303[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 2 task instances ready to be queued[0m
[[34m2021-04-05 07:30:01,303[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:30:01,303[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG fec_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:30:01,303[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:20:00+00:00 [scheduled]>
	<TaskInstance: fec_lead_importer.main 2021-04-05 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:30:01,325[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:30:01,325[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:30:01,326[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='fec_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 20, tzinfo=Timezone('UTC')), try_number=1) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:30:01,326[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:30:01,331[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:30:01,333[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'fec_lead_importer', 'main', '2021-04-05T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/airflow_poc.py'][0m
[[34m2021-04-05 07:30:01,672[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
[[34m2021-04-05 07:30:01,687[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/airflow_poc.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T07:20:00+00:00 [queued]> on host airflowvm
{'script': '$HOME/workspace/cron-jobs/scripts/standard/leads_importer/fec_importer.sh', 'client': 'fec', 'lastDays': '0', 'upsert': 'true'}
Running <TaskInstance: fec_lead_importer.main 2021-04-05T07:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:30:03,635[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of fec_lead_importer.main execution_date=2021-04-05 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:30:04,760[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun fec_lead_importer @ 2021-04-05 07:20:00+00:00: scheduled__2021-04-05T07:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:30:13,296[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 07:20:00+00:00 exited with status success for try_number 1[0m
[[34m2021-04-05 07:30:18,454[0m] {[34mscheduler_job.py:[0m938} INFO[0m - 1 tasks up for execution:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:30:18,455[0m] {[34mscheduler_job.py:[0m967} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 112 open slots and 1 task instances ready to be queued[0m
[[34m2021-04-05 07:30:18,455[0m] {[34mscheduler_job.py:[0m995} INFO[0m - DAG baxa_lead_importer has 0/16 running and queued tasks[0m
[[34m2021-04-05 07:30:18,455[0m] {[34mscheduler_job.py:[0m1060} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: baxa_lead_importer.main 2021-04-05 07:20:00+00:00 [scheduled]>[0m
[[34m2021-04-05 07:30:18,457[0m] {[34mscheduler_job.py:[0m1102} INFO[0m - Sending TaskInstanceKey(dag_id='baxa_lead_importer', task_id='main', execution_date=datetime.datetime(2021, 4, 5, 7, 20, tzinfo=Timezone('UTC')), try_number=2) to executor with priority 1 and queue default[0m
[[34m2021-04-05 07:30:18,457[0m] {[34mbase_executor.py:[0m79} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:30:18,459[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'baxa_lead_importer', 'main', '2021-04-05T07:20:00+00:00', '--local', '--pool', 'default_pool', '--subdir', '/home/airflow/airflow/dags/baxa_lead_importer.py'][0m
[[34m2021-04-05 07:30:18,688[0m] {[34mdagbag.py:[0m440} INFO[0m - Filling up the DagBag from /home/airflow/airflow/dags/baxa_lead_importer.py[0m
Running <TaskInstance: baxa_lead_importer.main 2021-04-05T07:20:00+00:00 [queued]> on host airflowvm
[[34m2021-04-05 07:30:22,760[0m] {[34mscheduler_job.py:[0m1751} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2021-04-05 07:30:29,914[0m] {[34mscheduler_job.py:[0m1193} INFO[0m - Executor reports execution of baxa_lead_importer.main execution_date=2021-04-05 07:20:00+00:00 exited with status success for try_number 2[0m
[[34m2021-04-05 07:30:30,988[0m] {[34mdagrun.py:[0m429} ERROR[0m - Marking run <DagRun baxa_lead_importer @ 2021-04-05 07:20:00+00:00: scheduled__2021-04-05T07:20:00+00:00, externally triggered: False> failed[0m
[[34m2021-04-05 07:32:43,616[0m] {[34mscheduler_job.py:[0m746} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2021-04-05 07:32:44,627[0m] {[34mprocess_utils.py:[0m95} INFO[0m - Sending Signals.SIGTERM to GPID 353813[0m
[2021-04-05 07:32:44,825] {process_utils.py:201} INFO - Waiting up to 5 seconds for processes to exit...
[[34m2021-04-05 07:32:44,833[0m] {[34mprocess_utils.py:[0m61} INFO[0m - Process psutil.Process(pid=387064, status='terminated', started='07:32:43') (387064) terminated with exit code None[0m
[[34m2021-04-05 07:32:44,835[0m] {[34mprocess_utils.py:[0m61} INFO[0m - Process psutil.Process(pid=353813, status='terminated', exitcode=0, started='07:20:05') (353813) terminated with exit code 0[0m
[[34m2021-04-05 07:32:44,836[0m] {[34mprocess_utils.py:[0m61} INFO[0m - Process psutil.Process(pid=387087, status='terminated', started='07:32:44') (387087) terminated with exit code None[0m
[[34m2021-04-05 07:32:44,842[0m] {[34mprocess_utils.py:[0m95} INFO[0m - Sending Signals.SIGTERM to GPID 353813[0m
[[34m2021-04-05 07:32:44,842[0m] {[34mscheduler_job.py:[0m1296} INFO[0m - Exited execute loop[0m
